{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that storing bigrams into a dictionary is highly inefficient,\n",
    "and it is also not very scalable.\n",
    "\"\"\"\n",
    "\n",
    "words\n",
    "\n",
    "b = {} # initialise dictionary\n",
    "\n",
    "for n in words:\n",
    "    char = ['<S>'] + list(n) + ['<E>'] # add starting and ending tags to identify start and end chars\n",
    "    for ch1,ch2 in zip(char, char[1:]):\n",
    "        bigram = ch1,ch2\n",
    "        b[bigram] = b.get(bigram, 0) +1 # search b if bigram exists else initialise to 0, then add 1 to the key\n",
    "\n",
    "b # print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing into a list is more efficient\n",
    "sorted(b.items(), key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Bigram Method:\n",
    "Using a 27x27 matrix to represent char1 x char2 bigram frequency count in dataset. This matrix will\n",
    "be used to calculate the target loss function of the dataset, which we will try to match through using\n",
    "the backpropagation.\n",
    "\"\"\"\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 27x27 torch matrix of zeros (26 alphabets + 1 \".\")\n",
    "N = torch.zeros((27,27) , dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the index of characters to index for the matrix\n",
    "charset = sorted(set(''.join(words)))\n",
    "charset.insert(0, \".\")\n",
    "stoi = { ltr: idx for idx, ltr in enumerate(charset) }\n",
    "itos = { idx: ltr for idx, ltr in enumerate(charset) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in words:\n",
    "    char = ['.'] + list(n) + ['.'] # add starting and ending tags to identify start and end chars\n",
    "    for ch1,ch2 in zip(char, char[1:]):\n",
    "        axi1 = stoi[ch1]\n",
    "        axi2 = stoi[ch2]\n",
    "        N[axi1, axi2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the frequency counts of <char1><char2> in the dataset\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        # ha = horizontal align, va = vertical align\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the dataset for \n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "# note that the arguments in sum are very important -> leads to very different results\n",
    "# print(P.sum(1, keepdim=True).shape)\n",
    "# print(P.sum(1).shape)\n",
    "# print(P.sum().shape)\n",
    "\n",
    "# additionally, take note of broadcasting rule\n",
    "# here, we are dividing P, a 27x27 matrix, with P.sum(args), a 27x1 matrix\n",
    "# in pytorch, broadcasting gives what we want, where the 27x1 matrix is \"stretched\"\n",
    "# into a 27x27x matrix, where the inputs to the first col are copied to the other cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = P[ix]\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0478\n",
      "em: 0.0377\n",
      "mm: 0.0253\n",
      "ma: 0.3885\n",
      "a.: 0.1958\n",
      ".o: 0.0123\n",
      "ol: 0.0779\n",
      "li: 0.1774\n",
      "iv: 0.0152\n",
      "vi: 0.3508\n",
      "ia: 0.1380\n",
      "a.: 0.1958\n",
      ".a: 0.1376\n",
      "av: 0.0246\n",
      "va: 0.2473\n",
      "a.: 0.1958\n"
     ]
    }
   ],
   "source": [
    "for w in words[:3]:\n",
    "    char = ['.'] + list(w) + ['.'] # add starting and ending tags to identify start and end chars\n",
    "    for ch1,ch2 in zip(char, char[1:]):\n",
    "        axi1 = stoi[ch1]\n",
    "        axi2 = stoi[ch2]\n",
    "        prob = P[axi1, axi2]\n",
    "        print(f'{ch1}{ch2}: {prob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll: tensor(-559951.5625)\n",
      "nll: tensor(2.4544)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section helps us determine the \"ground-truth\" of our dataset, using the\n",
    "log-likelihood estimator to calculate the \"loss\" for our dataset using the \n",
    "frequency of each bigram pairing.\n",
    "In the subsequent backpropagation modules, we aim to train the model to obtain\n",
    "a \"loss\" that is as close to the ground-truth as possible.\n",
    "\"\"\"\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    char = ['.'] + list(w) + ['.'] # add starting and ending tags to identify start and end chars\n",
    "    for ch1,ch2 in zip(char, char[1:]):\n",
    "        axi1 = stoi[ch1]\n",
    "        axi2 = stoi[ch2]\n",
    "        prob = P[axi1, axi2]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "print(\"ll:\",log_likelihood)\n",
    "nll = - log_likelihood / n\n",
    "print(\"nll:\",nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We start by creating the training set of input char x, and target char y\n",
    "\"\"\"\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for w in words[:1]:\n",
    "    char = ['.'] + list(w) + ['.'] # add starting and ending tags to identify start and end chars\n",
    "    for ch1,ch2 in zip(char, char[1:]):\n",
    "        axi1 = stoi[ch1]\n",
    "        axi2 = stoi[ch2]\n",
    "        xs.append(axi1)\n",
    "        ys.append(axi2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faaf6b927f0>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first initialize a matrix with 1 column x 27 rows\n",
    "# this is equivalent of our random weights in prev lesson\n",
    "# we will backpropagate and optimize this W\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0261, 0.0193, 0.0255, 0.0191, 0.0466, 0.0322, 0.0252, 0.0140, 0.1186,\n",
       "         0.3459, 0.0165, 0.0041, 0.0035, 0.0074, 0.0115, 0.0521, 0.0104, 0.0105,\n",
       "         0.0349, 0.0144, 0.0417, 0.0222, 0.0238, 0.0178, 0.0196, 0.0298, 0.0074],\n",
       "        [0.0163, 0.0010, 0.0102, 0.0101, 0.0032, 0.0035, 0.0115, 0.0209, 0.0077,\n",
       "         0.0902, 0.0742, 0.0592, 0.0722, 0.0414, 0.0497, 0.0566, 0.0383, 0.0175,\n",
       "         0.0447, 0.1374, 0.0522, 0.0162, 0.0225, 0.0083, 0.0356, 0.0433, 0.0560],\n",
       "        [0.0288, 0.0822, 0.0242, 0.0651, 0.0242, 0.0313, 0.0224, 0.0248, 0.0508,\n",
       "         0.0132, 0.0185, 0.0435, 0.0208, 0.0459, 0.0203, 0.0200, 0.0723, 0.0315,\n",
       "         0.0868, 0.0374, 0.1300, 0.0051, 0.0081, 0.0115, 0.0118, 0.0243, 0.0450],\n",
       "        [0.0288, 0.0822, 0.0242, 0.0651, 0.0242, 0.0313, 0.0224, 0.0248, 0.0508,\n",
       "         0.0132, 0.0185, 0.0435, 0.0208, 0.0459, 0.0203, 0.0200, 0.0723, 0.0315,\n",
       "         0.0868, 0.0374, 0.1300, 0.0051, 0.0081, 0.0115, 0.0118, 0.0243, 0.0450],\n",
       "        [0.0189, 0.0086, 0.0984, 0.0095, 0.0330, 0.0169, 0.0361, 0.0284, 0.0489,\n",
       "         0.1245, 0.0267, 0.0033, 0.0049, 0.1299, 0.0064, 0.0318, 0.0212, 0.0067,\n",
       "         0.0045, 0.0315, 0.0109, 0.1562, 0.0091, 0.0364, 0.0042, 0.0157, 0.0772]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conduct matrix multiplication with our xenc matrix dataset\n",
    "logits = xenc @ W\n",
    "counts = logits.exp() # this is equivalent to our N above\n",
    "probability = counts / counts.sum(1, keepdim=True) # this and the above are\n",
    "# the equiv of the 'softmax activation function'\n",
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll of .e tensor(3.4361)\n",
      "nll of em tensor(3.1850)\n",
      "nll of mm tensor(3.0814)\n",
      "nll of ma tensor(2.4983)\n",
      "nll of a. tensor(3.9672)\n"
     ]
    }
   ],
   "source": [
    "# now, taking the original xs and ys inputs and targets, and our W\n",
    "# matrix of predicted probabilities, we calculate the loss function\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    prob = probability[i,y]\n",
    "    logprob = torch.log(prob)\n",
    "    negll = - logprob\n",
    "    nlls[i] = negll\n",
    "    print(f'nll of {itos[x]}{itos[y]}',negll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- full code here ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data: 228146\n"
     ]
    }
   ],
   "source": [
    "# initialize dataset\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    char = ['.'] + list(w) + ['.'] # add starting and ending tags to identify start and end chars\n",
    "    for ch1,ch2 in zip(char, char[1:]):\n",
    "        axi1 = stoi[ch1]\n",
    "        axi2 = stoi[ch2]\n",
    "        xs.append(axi1)\n",
    "        ys.append(axi2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "numElements = xs.nelement()\n",
    "print(\"number of training data:\", numElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize neural net\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is:2.4576191902160645\n",
      "loss is:2.4575998783111572\n",
      "loss is:2.45758056640625\n",
      "loss is:2.457561731338501\n",
      "loss is:2.457542896270752\n",
      "loss is:2.457524538040161\n",
      "loss is:2.457505941390991\n",
      "loss is:2.4574878215789795\n",
      "loss is:2.4574697017669678\n",
      "loss is:2.457451820373535\n",
      "loss is:2.4574339389801025\n",
      "loss is:2.457416534423828\n",
      "loss is:2.457399368286133\n",
      "loss is:2.4573822021484375\n",
      "loss is:2.457365036010742\n",
      "loss is:2.457348108291626\n",
      "loss is:2.457331657409668\n",
      "loss is:2.457314968109131\n",
      "loss is:2.457298755645752\n",
      "loss is:2.457282304763794\n",
      "loss is:2.457266092300415\n",
      "loss is:2.4572501182556152\n",
      "loss is:2.4572346210479736\n",
      "loss is:2.457218647003174\n",
      "loss is:2.4572033882141113\n",
      "loss is:2.457188129425049\n",
      "loss is:2.4571728706359863\n",
      "loss is:2.457157611846924\n",
      "loss is:2.4571428298950195\n",
      "loss is:2.4571282863616943\n",
      "loss is:2.45711350440979\n",
      "loss is:2.4570987224578857\n",
      "loss is:2.4570844173431396\n",
      "loss is:2.4570698738098145\n",
      "loss is:2.4570560455322266\n",
      "loss is:2.4570415019989014\n",
      "loss is:2.4570281505584717\n",
      "loss is:2.457014322280884\n",
      "loss is:2.457000732421875\n",
      "loss is:2.456986904144287\n",
      "loss is:2.4569737911224365\n",
      "loss is:2.456961154937744\n",
      "loss is:2.4569501876831055\n",
      "loss is:2.456942558288574\n",
      "loss is:2.4569456577301025\n",
      "loss is:2.456979274749756\n",
      "loss is:2.4570937156677246\n",
      "loss is:2.4574623107910156\n",
      "loss is:2.4584105014801025\n",
      "loss is:2.4614083766937256\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for i in range(50):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp() # this is equivalent to our N above\n",
    "    probability = counts / counts.sum(1, keepdim=True) # this and the above are\n",
    "    # the equiv of the 'softmax activation function'\n",
    "    loss = -probability[torch.arange(numElements), ys].log().mean()\n",
    "    print(f'loss is:{loss}')\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # setting gradients to 0\n",
    "    loss.backward()\n",
    "\n",
    "    # perturbation\n",
    "    W.data += -100 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "prelay.\n",
      "a.\n",
      "nn.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
